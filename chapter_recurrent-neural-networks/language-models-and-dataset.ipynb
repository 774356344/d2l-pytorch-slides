{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Language Model Dataset\n",
    "\n",
    "Read minibatches of input sequences and label sequences at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "origin_pos": 4,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "corpus, vocab = d2l.load_corpus_time_machine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class SeqDataLoader:  \n",
    "    \"\"\"The sequence data iterator generating minibatches of subsequences.\"\"\"\n",
    "    def __init__(self, corpus, batch_size, num_steps):\n",
    "        self.corpus, self.b, self.n = corpus, batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus = self.corpus[random.randint(0, self.n - 1):]\n",
    "        m = (len(corpus) - 1) // self.n\n",
    "        initial_indices = list(range(0, m * self.n, self.n))\n",
    "        random.shuffle(initial_indices)\n",
    "        for i in range(0, m // self.b):\n",
    "            batch_indicies = initial_indices[i * self.b:(i + 1) * self.b]\n",
    "            X = [corpus[j:j + self.n] for j in batch_indicies]\n",
    "            Y = [corpus[j + 1:j + 1 + self.n] for j in batch_indicies]\n",
    "            yield torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Manually generate a sequence from 0 to 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28],\n",
      "        [ 4,  5,  6,  7,  8]]) \n",
      "Y: tensor([[20, 21, 22, 23, 24],\n",
      "        [25, 26, 27, 28, 29],\n",
      "        [ 5,  6,  7,  8,  9]])\n",
      "X:  tensor([[29, 30, 31, 32, 33],\n",
      "        [ 9, 10, 11, 12, 13],\n",
      "        [14, 15, 16, 17, 18]]) \n",
      "Y: tensor([[30, 31, 32, 33, 34],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19]])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in SeqDataLoader(list(range(35)), batch_size=3, num_steps=5):\n",
    "    print('X: ', X, '\\nY:', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Last, we define a function `load_data_time_machine` that returns both the data iterator and the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def load_data_time_machine(batch_size, num_steps, max_tokens=10000):  \n",
    "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
    "    corpus, vocab = d2l.load_corpus_time_machine(max_tokens)\n",
    "    data_iter = SeqDataLoader(corpus, batch_size, num_steps)\n",
    "    return data_iter, vocab"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "language_info": {
   "name": "python"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
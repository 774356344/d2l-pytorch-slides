{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Language Model Dataset\n",
    "\n",
    "Read minibatches of input sequences and label sequences at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.TimeMachine)  \n",
    "def __init__(self, batch_size, num_steps, num_train=10000, num_val=5000):\n",
    "    super(d2l.TimeMachine, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.prepare_data()\n",
    "\n",
    "class LMDataLoader(d2l.HyperParameters):  \n",
    "    def __init__(self, corpus, batch_size, num_steps, train):\n",
    "        self.save_hyperparameters()\n",
    "        self.num_batches = (len(corpus) - 1 - (num_steps if train else 0)\n",
    "                           ) // (self.num_steps * self.batch_size)\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus = (self.corpus[random.randint(0, self.num_steps - 1):]\n",
    "                  if self.train else self.corpus)\n",
    "        m = (len(corpus)-1) // self.num_steps\n",
    "        initial_indices = list(range(0, m*self.num_steps, self.num_steps))\n",
    "        if self.train:\n",
    "            random.shuffle(initial_indices)\n",
    "        for i in range(0, self.num_batches):\n",
    "            batch_indicies = initial_indices[\n",
    "                i*self.batch_size : (i+1) * self.batch_size]\n",
    "            X = [corpus[j : j+self.num_steps] for j in batch_indicies]\n",
    "            Y = [corpus[j+1 : j+1+self.num_steps] for j in batch_indicies]\n",
    "            yield torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "\n",
    "@d2l.add_to_class(d2l.TimeMachine)  \n",
    "def get_dataloader(self, train):\n",
    "    corpus = (self.corpus[: self.num_train] if train else\n",
    "              self.corpus[self.num_train : self.num_train+self.num_val])\n",
    "    return LMDataLoader(corpus, self.batch_size, self.num_steps, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Manually generate a sequence from 0 to 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    },
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[24,  9, 26, 20,  9, 16, 22, 13,  5,  0],\n",
      "        [ 2, 19,  6, 15, 21,  4, 19, 26, 20, 21]]) \n",
      "Y: tensor([[ 9, 26, 20,  9, 16, 22, 13,  5,  0,  9],\n",
      "        [19,  6, 15, 21,  4, 19, 26, 20, 21,  2]])\n"
     ]
    }
   ],
   "source": [
    "data = d2l.TimeMachine(batch_size=2, num_steps=10)\n",
    "for X, Y in data.train_dataloader():\n",
    "    print('X:', X, '\\nY:', Y)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "language_info": {
   "name": "python"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}